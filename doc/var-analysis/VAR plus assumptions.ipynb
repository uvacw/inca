{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector AutoRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class inherets from Base_Analysis_Class and provides some simple functionality for: \n",
    "    1. Checking stationarity of (multiple) time-series\n",
    "    \n",
    "    2. Plotting timeline, lag-plot, autocorrelation plot (you need to be on ipython environment)\n",
    "    \n",
    "    3. Creating and fitting VAR model based on the dataframe (index=time variable) provided from the 'timeline' class. Here, you can speicfy:\n",
    "        - N-lags /ByDefault statsmodels chooses the most apporporate lag for you based on Ljung-Box             Q-score. \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "bobber = poplvok :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Blueprint of the class VAR \n",
    "#requirements: \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from core.analysis_base_class import Analysis\n",
    "from analysis import timeline_analysis as ta\n",
    "from statsmodels.tsa.api import VAR as var \n",
    "from statsmodels.tsa.stattools import adfuller,kpss\n",
    "from statsmodels.tsa.tsatools import detrend\n",
    "from matplotlib import pyplot\n",
    "from pandas.plotting import lag_plot\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "\n",
    "class VAR(Analysis):\n",
    "    \"\"\" When creating var model we first generate a timeline in the form of pandas df. Then feed it to VAR method in statsmodels.\n",
    "        We can save the names of the variables(queries) this way so no need of mapping (var to name) on the later stages,\n",
    "        awesome plotting functionality and just comfortable to work with for everyone. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,queries,timefield,granularity, querytype=\"count\"):\n",
    "        \"\"\" @queries  = what do you want to query from ES ? eg queries = ['de','het']\n",
    "            @timefield = what field do you want to use to get the dates/timeline from ? 'META.ADDED'\n",
    "            @ granularity = 'day'/'week'/'month' etc \n",
    "            \n",
    "            We need a nice dataframe with (index=time & cols=queries) to work further - so lets create it here \n",
    "        \"\"\"\n",
    "        timeline = ta.timeline_generator()\n",
    "        df_raw = timeline.analyse(queries=queries,timefield = timefield, granularity = granularity)\n",
    "        df_raw.index = df_raw.timestamp\n",
    "        df_raw = df_raw.drop('timestamp',axis=1)\n",
    "        # by default the timeline is made balanced ...\n",
    "        # check for issues with series at this step ? \n",
    "        self.df = df_raw\n",
    "        self.temp = 0.0   ##\n",
    "          \n",
    "\n",
    "    def test_assumptions(self,level = '5%',**kwargs):\n",
    "        \"\"\" \n",
    "        Gives you output as list with assumptions stated as satisfied/not satisfied. If some important \n",
    "        assumptions are not satisfied gives you warning that you have to transform your data. \n",
    "        \n",
    "        @df - 'active' df which we might want to modify\n",
    "        @level -  this is the level you are testing your asusmptions on (either 1,5 or 10 %)\n",
    "        \"\"\"\n",
    "        self.level =  level\n",
    "        df = self.df          ## \n",
    "        \n",
    "        \n",
    "        \n",
    "        def _adf_test(df):\n",
    "            \"\"\" \n",
    "            H_0: the observed time series is stationary \n",
    "            Returns: dataframe of summary of the test \n",
    "            \"\"\"\n",
    "            summary_adf = pd.DataFrame(columns=['ADF_Stat','p-value','Critical_val_1%','Critical_val_5%','Critical_val_10%'])\n",
    "            for name in df.columns:\n",
    "                series = df[name]\n",
    "                result = adfuller(series)\n",
    "                dic = {'ADF_Stat':result[0],'p-value':result[1],'Critical_val_1%':result[4]['1%'],'Critical_val_5%':result[4]['5%'],\n",
    "                       'Critical_val_10%':result[4]['10%']}\n",
    "                summary_adf = summary_adf.append(dic,ignore_index=True)\n",
    "            summary_adf.set_index(df.columns,inplace=True)  \n",
    "\n",
    "            return summary_adf \n",
    "    \n",
    "        def _kpss_test(df):\n",
    "            \"\"\" \n",
    "            H_0: there is a unit root in time series, hence stochastic trend with drift, hence non-stationary\n",
    "            Returns: dataframe of summary of the test\n",
    "            \"\"\"\n",
    "            summary_kpss = pd.DataFrame(columns=['KPSS_Stat','p-value','Critical_val_1%','Critical_val_5%','Critical_val_10%'])\n",
    "            for name in df.columns:\n",
    "                series = df[name]\n",
    "                result = kpss(series)\n",
    "                dic = {'KPSS_Stat':result[0],'p-value':result[1],'Critical_val_1%':result[3]['1%'],'Critical_val_5%':result[3]['5%'],\n",
    "                       'Critical_val_10%':result[3]['10%']}\n",
    "                summary_kpss = summary_kpss.append(dic,ignore_index=True)\n",
    "            summary_kpss.set_index(df.columns,inplace=True)  \n",
    "\n",
    "            return summary_kpss\n",
    "        \n",
    "        def _stationary(df, explicit=True):\n",
    "            \"\"\" \n",
    "            For each time series return the result of the check - return in created dataframe ?\n",
    "            \"\"\"\n",
    "            lvl = float(self.level[:-1])/100 \n",
    "            self.summary_adf = _adf_test(df)\n",
    "            self.summary_kpss = _kpss_test(df)\n",
    "             \n",
    "            stat_flag = True       ## if one of the test failed, we assume we need to take action \n",
    "            if explicit:\n",
    "                for i in df.columns:\n",
    "                    adf_flag = lvl > self.summary_adf.loc[i,'p-value']\n",
    "                    kpss_flag = lvl < self.summary_kpss.loc[i,'p-value']\n",
    "                    if ((adf_flag == False) | (kpss_flag == False)):\n",
    "                        stat_flag = False \n",
    "                    print(\"For {} stationarity is satisfied: ADF - {} | KPSS - {} \".format(i,adf_flag, kpss_flag)) # PRINT  (!)\n",
    "                    \n",
    "            return stat_flag   \n",
    "        \n",
    "        def differencing(on=df, order=1):\n",
    "            \"\"\" If there is no stationarity: try differencing\n",
    "            \"\"\"             \n",
    "            def _perform_differencing():\n",
    "            # Perform differencing:  \n",
    "                df_diff = pd.DataFrame(columns=df.columns)\n",
    "                for name in df.columns:\n",
    "                    series = df[name]\n",
    "                    series = series.diff(order)\n",
    "                    df_diff[name] = series.dropna(axis=0)\n",
    "                \n",
    "                return _stationary(df_diff)   \n",
    "            \n",
    "            stat_check_after_diff = _perform_differencing()\n",
    "            print('Differencing helped?')   ##            \n",
    "            print(stat_check_after_diff)   ##\n",
    "            \n",
    "            ## if differencing helped update self.df => self.df = df_diff \n",
    "            \n",
    "            return ##\n",
    "        \n",
    "        def detrending(on=df, order=1):\n",
    "            \"\"\" If there is no stationarity: differencing did not help - detrend\n",
    "            \n",
    "                (!) does not work nicely with time series where there is a lot of zeros and high volatility :(\n",
    "            \"\"\"             \n",
    "            def _perform_detrending():\n",
    "            # Perform detrending:  \n",
    "                df_res = pd.DataFrame(columns=df.columns)\n",
    "                for name in df.columns:\n",
    "                    series = df[name]\n",
    "                    res = detrend(series,order)\n",
    "                    df_res[name] = series.dropna(axis=0)\n",
    "                \n",
    "                return _stationary(df_res)   \n",
    "            \n",
    "            stat_check_after_detrend = _perform_detrending()\n",
    "            print('Detrending helped?')   ##            \n",
    "            print(stat_check_after_detrend)   ##\n",
    "            \n",
    "            ## if differencing helped update self.df => self.df = df_diff \n",
    "            \n",
    "            return ##\n",
    "        \n",
    "        \n",
    "        stationarity_check = _stationary(df, explicit=True)  ## if false then one one or more of the series might be non-stat.\n",
    "        ## if stat._check == False\n",
    "        \n",
    "        do_diff = differencing(df)\n",
    "        do_detrend = detrending(df)\n",
    "        return ##\n",
    "\n",
    "    def fit(self,df, nlags=None):\n",
    "        \"\"\"\n",
    "        This method creates a Vector AutoRegressive model to timeline dataframe\n",
    "        @df - dataframe with columns representing queries (eg count number of documents \n",
    "               whcich mention a word or phrase) \n",
    "        @nlags -  number of lags to consider \n",
    "        \"\"\"\n",
    "        self.model = var(self.df)                           ## creating VAR model , could go to __init__\n",
    "        self.result = self.model.fit()   \n",
    "        #self.order = self.model.select_order(verbose=True)              ## this will automatically detect the best lag\n",
    "        #print(self.order['bic'])                                        ##selects lag based on infomation criteria\n",
    "        \n",
    "        return #0.0 #self.result.summary()###\n",
    "\n",
    "    def forecast(self,d = 5):\n",
    "        \"\"\"\n",
    "        Makes forecasts based on the parameters of fitted model \n",
    "        @ d - how many steps into the future you want to forecast \n",
    "        \"\"\"\n",
    "        prediction_array= self.result.forecast(self.df.values,d)\n",
    "        \n",
    "        predictions = pd.DataFrame(prediction_array, columns= self.df.columns)\n",
    "        print(predictions)     ###\n",
    "        return ## \n",
    "\n",
    "\n",
    "    def interpretation(self, **kwargs):\n",
    "        \"\"\"\n",
    "        This method should have the functionality to interpret the status of the model after being trained and also document the various design choices\\\n",
    "        (i.e. parameters settings, assumptions, model selection, test method, dataset used). For example it can return a report-like looking formatted string.\\n\n",
    "        Please consider the following as possible model state interpretation:\\n\n",
    "           * For classification tasks depending on the underlying model: coeficient/feature weights, feature selection (random forest)\\n\n",
    "           * For clustering tasks: clusterings members/structure, distributions\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def diagnostics(self):\n",
    "        \"\"\"\n",
    "        This method should have the functionality to report on the quality of the underlying (trained) model used for \n",
    "        the analysis (on a dataset)\n",
    "           \n",
    "        Common diagnostics for VAR: check noramlity of residuals\n",
    "        \"\"\"\n",
    "        residuals = self.result.resid\n",
    "        print(residuals)\n",
    "        return ##\n",
    "\n",
    "    def plot(self, plot_type=None, lag = 1):\n",
    "        \"\"\"\n",
    "        To be able to see the results this method requires an ipython environment run\n",
    "        \"\"\"\n",
    "        def lag_scatter():\n",
    "            for name in self.df.columns:\n",
    "                series = self.df[name]\n",
    "                print(\"Lag plot where y is {}\".format(name))\n",
    "                lag_plot(series,lag)\n",
    "                pyplot.show()    \n",
    "\n",
    "            return ##\n",
    "        \n",
    "        def line_plot():\n",
    "            for name in self.df.columns:\n",
    "                series = self.df[name]\n",
    "                series.plot(legend=True)\n",
    "                pyplot.show()\n",
    "            return ##\n",
    "    \n",
    "        def autocorrelation_plot():\n",
    "            for name in self.df.columns:\n",
    "                series = self.df[name]\n",
    "                print(\"Autocorelation plot for {}\".format(name))\n",
    "                plot_acf(series, lags=lag)\n",
    "                pyplot.show()\n",
    "            \n",
    "        if plot_type == None:\n",
    "            lag_scatter() \n",
    "            line_plot()\n",
    "            autocorrelation_plot()\n",
    "        if (plot_type == ('line')):\n",
    "            line_plot()\n",
    "        if (plot_type == ('lag')):\n",
    "            lag_scatter()\n",
    "        if (plot_type == ('autocorrelation')):\n",
    "            autocorrelation_plot()\n",
    "        \n",
    "        return ## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "d = input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import inca "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m1 = VAR(queries=['de','het'], timefield = 'META.ADDED', granularity = 'day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1. de</th>\n",
       "      <th>2. het</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-02T00:00:00.000Z</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03T00:00:00.000Z</th>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-04T00:00:00.000Z</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05T00:00:00.000Z</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06T00:00:00.000Z</th>\n",
       "      <td>199</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-07T00:00:00.000Z</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-08T00:00:00.000Z</th>\n",
       "      <td>143</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-09T00:00:00.000Z</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-10T00:00:00.000Z</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-11T00:00:00.000Z</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-12T00:00:00.000Z</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-13T00:00:00.000Z</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14T00:00:00.000Z</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-15T00:00:00.000Z</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-16T00:00:00.000Z</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-17T00:00:00.000Z</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-18T00:00:00.000Z</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-19T00:00:00.000Z</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-20T00:00:00.000Z</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-21T00:00:00.000Z</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-22T00:00:00.000Z</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-23T00:00:00.000Z</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-24T00:00:00.000Z</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-25T00:00:00.000Z</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-26T00:00:00.000Z</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          1. de     2. het   \n",
       "timestamp                                    \n",
       "2017-11-02T00:00:00.000Z        10         10\n",
       "2017-11-03T00:00:00.000Z        19         19\n",
       "2017-11-04T00:00:00.000Z         0          0\n",
       "2017-11-05T00:00:00.000Z         0          0\n",
       "2017-11-06T00:00:00.000Z       199        198\n",
       "2017-11-07T00:00:00.000Z         0          0\n",
       "2017-11-08T00:00:00.000Z       143        143\n",
       "2017-11-09T00:00:00.000Z         0          0\n",
       "2017-11-10T00:00:00.000Z         0          0\n",
       "2017-11-11T00:00:00.000Z         0          0\n",
       "2017-11-12T00:00:00.000Z         0          0\n",
       "2017-11-13T00:00:00.000Z         0          0\n",
       "2017-11-14T00:00:00.000Z         0          0\n",
       "2017-11-15T00:00:00.000Z         0          0\n",
       "2017-11-16T00:00:00.000Z         0          0\n",
       "2017-11-17T00:00:00.000Z         0          0\n",
       "2017-11-18T00:00:00.000Z         0          0\n",
       "2017-11-19T00:00:00.000Z         0          0\n",
       "2017-11-20T00:00:00.000Z         0          0\n",
       "2017-11-21T00:00:00.000Z         0          0\n",
       "2017-11-22T00:00:00.000Z         0          0\n",
       "2017-11-23T00:00:00.000Z         0          0\n",
       "2017-11-24T00:00:00.000Z         0          0\n",
       "2017-11-25T00:00:00.000Z         0          0\n",
       "2017-11-26T00:00:00.000Z        10         10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda2\\envs\\inca362\\lib\\site-packages\\statsmodels\\tsa\\stattools.py:1260: InterpolationWarning: p-value is greater than the indicated p-value\n",
      "  warn(\"p-value is greater than the indicated p-value\", InterpolationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 1. de    stationarity is satisfied: ADF - True | KPSS - True \n",
      "For 2. het    stationarity is satisfied: ADF - True | KPSS - True \n",
      "For 1. de    stationarity is satisfied: ADF - True | KPSS - True \n",
      "For 2. het    stationarity is satisfied: ADF - True | KPSS - True \n",
      "Differencing helped?\n",
      "True\n",
      "For 1. de    stationarity is satisfied: ADF - True | KPSS - True \n",
      "For 2. het    stationarity is satisfied: ADF - True | KPSS - True \n",
      "Detrending helped?\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "m1.test_assumptions('1%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "fit() missing 1 required positional argument: 'df'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-76b58544601c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mm1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mm1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforecast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mm1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplot_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'autocorrelation'\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mlag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#m1.diagnostics()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: fit() missing 1 required positional argument: 'df'"
     ]
    }
   ],
   "source": [
    "m1.fit()\n",
    "m1.forecast()\n",
    "m1.plot(plot_type = 'autocorrelation' ,lag = 2)\n",
    "#m1.diagnostics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Anaconda2\\envs\\inca362\\lib\\site-packages\\statsmodels\\compat\\pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.stattools import grangercausalitytests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate two arbitrary sample to work with, put them in df, add time trend\n",
    "sample1 = np.random.gamma(4,3,50)\n",
    "sample2 = np.random.binomial(10,0.7,size=50)\n",
    "sample3 = np.random.binomial(10,0.2,size=50)\n",
    "\n",
    "# create a pandas dataframe with counts of mentions of a particular word (two arrays)\n",
    "df = pd.DataFrame({'x':sample1, 'y':sample2, 'z':sample3},dtype='float')\n",
    "df['t'] = df.index\n",
    "#df['x'] = df.x.astype('int')\n",
    "df = df.drop(['t'],axis=1)\n",
    "\n",
    "#make index a date object\n",
    "import datetime\n",
    "base = datetime.datetime.today()\n",
    "date_list = [base - datetime.timedelta(days=x) for x in range(0, 50)]\n",
    "\n",
    "df.index = date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arr  = df.iloc[:,[0,2]]\n",
    "arr = arr.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 1\n",
      "ssr based F test:         F=0.2397  , p=0.6267  , df_denom=46, df_num=1\n",
      "ssr based chi2 test:   chi2=0.2554  , p=0.6133  , df=1\n",
      "likelihood ratio test: chi2=0.2547  , p=0.6138  , df=1\n",
      "parameter F test:         F=0.2397  , p=0.6267  , df_denom=46, df_num=1\n",
      "\n",
      "Granger Causality\n",
      "number of lags (no zero) 2\n",
      "ssr based F test:         F=0.4012  , p=0.6720  , df_denom=43, df_num=2\n",
      "ssr based chi2 test:   chi2=0.8958  , p=0.6390  , df=2\n",
      "likelihood ratio test: chi2=0.8875  , p=0.6416  , df=2\n",
      "parameter F test:         F=0.4012  , p=0.6720  , df_denom=43, df_num=2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.62672664057858518"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grangercausalitytests(arr,maxlag=2)[1][0]['ssr_ftest'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-11-15 12:57:19.273678</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-14 12:57:19.273678</th>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-13 12:57:19.273678</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-12 12:57:19.273678</th>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-11 12:57:19.273678</th>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-10 12:57:19.273678</th>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-09 12:57:19.273678</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-08 12:57:19.273678</th>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-07 12:57:19.273678</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-06 12:57:19.273678</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-05 12:57:19.273678</th>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-04 12:57:19.273678</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-03 12:57:19.273678</th>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-02 12:57:19.273678</th>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-11-01 12:57:19.273678</th>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-31 12:57:19.273678</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-30 12:57:19.273678</th>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-29 12:57:19.273678</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-28 12:57:19.273678</th>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-27 12:57:19.273678</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-26 12:57:19.273678</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-25 12:57:19.273678</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-24 12:57:19.273678</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-23 12:57:19.273678</th>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-22 12:57:19.273678</th>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-21 12:57:19.273678</th>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-20 12:57:19.273678</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-19 12:57:19.273678</th>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-18 12:57:19.273678</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-17 12:57:19.273678</th>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-16 12:57:19.273678</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-15 12:57:19.273678</th>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-14 12:57:19.273678</th>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-13 12:57:19.273678</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-12 12:57:19.273678</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-11 12:57:19.273678</th>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-10 12:57:19.273678</th>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-09 12:57:19.273678</th>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-08 12:57:19.273678</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-07 12:57:19.273678</th>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-06 12:57:19.273678</th>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-05 12:57:19.273678</th>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-04 12:57:19.273678</th>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-03 12:57:19.273678</th>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-02 12:57:19.273678</th>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-10-01 12:57:19.273678</th>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-30 12:57:19.273678</th>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-29 12:57:19.273678</th>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-28 12:57:19.273678</th>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-09-27 12:57:19.273678</th>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             x   y\n",
       "2017-11-15 12:57:19.273678   3   9\n",
       "2017-11-14 12:57:19.273678  13   6\n",
       "2017-11-13 12:57:19.273678   8   9\n",
       "2017-11-12 12:57:19.273678   8  10\n",
       "2017-11-11 12:57:19.273678  11   7\n",
       "2017-11-10 12:57:19.273678  11   6\n",
       "2017-11-09 12:57:19.273678   6   7\n",
       "2017-11-08 12:57:19.273678  20   7\n",
       "2017-11-07 12:57:19.273678  10   8\n",
       "2017-11-06 12:57:19.273678   7  10\n",
       "2017-11-05 12:57:19.273678  18   6\n",
       "2017-11-04 12:57:19.273678   3   8\n",
       "2017-11-03 12:57:19.273678  20   8\n",
       "2017-11-02 12:57:19.273678  18   7\n",
       "2017-11-01 12:57:19.273678   6   7\n",
       "2017-10-31 12:57:19.273678  10   6\n",
       "2017-10-30 12:57:19.273678  13   8\n",
       "2017-10-29 12:57:19.273678   5   6\n",
       "2017-10-28 12:57:19.273678  14   7\n",
       "2017-10-27 12:57:19.273678   6   6\n",
       "2017-10-26 12:57:19.273678   6   9\n",
       "2017-10-25 12:57:19.273678   2   8\n",
       "2017-10-24 12:57:19.273678   8   8\n",
       "2017-10-23 12:57:19.273678  33   7\n",
       "2017-10-22 12:57:19.273678   7   5\n",
       "2017-10-21 12:57:19.273678  12   8\n",
       "2017-10-20 12:57:19.273678   7   6\n",
       "2017-10-19 12:57:19.273678  21   5\n",
       "2017-10-18 12:57:19.273678   7   7\n",
       "2017-10-17 12:57:19.273678   5   8\n",
       "2017-10-16 12:57:19.273678   5   6\n",
       "2017-10-15 12:57:19.273678   8   9\n",
       "2017-10-14 12:57:19.273678   9   8\n",
       "2017-10-13 12:57:19.273678   6   9\n",
       "2017-10-12 12:57:19.273678   1   7\n",
       "2017-10-11 12:57:19.273678  11   7\n",
       "2017-10-10 12:57:19.273678  16   7\n",
       "2017-10-09 12:57:19.273678  13   6\n",
       "2017-10-08 12:57:19.273678   9   7\n",
       "2017-10-07 12:57:19.273678  19   5\n",
       "2017-10-06 12:57:19.273678  10   8\n",
       "2017-10-05 12:57:19.273678  14   8\n",
       "2017-10-04 12:57:19.273678  14   8\n",
       "2017-10-03 12:57:19.273678  13   9\n",
       "2017-10-02 12:57:19.273678  11  10\n",
       "2017-10-01 12:57:19.273678   5  10\n",
       "2017-09-30 12:57:19.273678   8   5\n",
       "2017-09-29 12:57:19.273678  12   9\n",
       "2017-09-28 12:57:19.273678  16   9\n",
       "2017-09-27 12:57:19.273678   9   7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inca362",
   "language": "python",
   "name": "inca362"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
